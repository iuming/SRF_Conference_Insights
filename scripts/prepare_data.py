#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†è„šæœ¬ - ä¸ºGitHub Pageså‡†å¤‡æ•°æ®
å°†æå–çš„PDFå†…å®¹è½¬æ¢ä¸ºå‰ç«¯å¯ç”¨çš„JSONæ ¼å¼
"""

import json
import os
import sys
import shutil
from pathlib import Path
import base64
from datetime import datetime

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ - ä¸ºå‰ç«¯å‡†å¤‡æ•°æ®"""
    
    def __init__(self, source_dir=".", output_dir="docs"):
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        self.data_dir = self.output_dir / "data"
        self.images_dir = self.output_dir / "images"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.images_dir.mkdir(parents=True, exist_ok=True)
    
    def process_all_data(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        print("ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®...")
        
        # 1. æŸ¥æ‰¾å¹¶å¤„ç†JSONæ•°æ®æ–‡ä»¶
        json_files = list(self.source_dir.glob("content_extraction_full_*.json"))
        if not json_files:
            print("âŒ æœªæ‰¾åˆ°å†…å®¹æå–çš„JSONæ–‡ä»¶")
            return False
        
        latest_json = max(json_files, key=os.path.getctime)
        print(f"ğŸ“„ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {latest_json}")
        
        # 2. åŠ è½½å’Œå¤„ç†è®ºæ–‡æ•°æ®
        with open(latest_json, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        papers_data = self.process_papers_data(data)
        
        # 3. ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.save_papers_data(papers_data)
        
        # 4. å¤„ç†å›¾ç‰‡
        self.process_images()
        
        # 5. ç”Ÿæˆç»Ÿè®¡æ•°æ®
        self.generate_statistics(papers_data)
        
        # 6. å¤åˆ¶å¿…è¦çš„æ–‡ä»¶
        self.copy_static_files()
        
        print("âœ… æ•°æ®å¤„ç†å®Œæˆ!")
        return True
    
    def process_papers_data(self, data):
        """å¤„ç†è®ºæ–‡æ•°æ®"""
        papers = data.get('papers', [])
        processed_papers = []
        
        print(f"ğŸ“š å¤„ç† {len(papers)} ç¯‡è®ºæ–‡...")
        
        for paper in papers:
            processed_paper = {
                'paper_number': paper.get('paper_number', 0),
                'filename': paper.get('filename', ''),
                'title': paper.get('title', ''),
                'authors': paper.get('authors', []),
                'affiliations': paper.get('affiliations', []),
                'abstract': paper.get('abstract', ''),
                'keywords': paper.get('keywords', []),
                'page_count': paper.get('page_count', 0),
                'file_size_kb': paper.get('file_size_kb', 0),
                'figures': paper.get('figures', []),
                'tables': paper.get('tables', []),
                'references': paper.get('references', []),
                'sections': paper.get('sections', {}),
                'reference_count': len(paper.get('references', [])),
                'figure_count': len(paper.get('figures', [])),
                'table_count': len(paper.get('tables', []))
            }
            processed_papers.append(processed_paper)
        
        return {
            'extraction_time': data.get('extraction_time', ''),
            'total_papers': len(processed_papers),
            'papers': processed_papers
        }
    
    def save_papers_data(self, papers_data):
        """ä¿å­˜è®ºæ–‡æ•°æ®"""
        output_file = self.data_dir / "papers.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(papers_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è®ºæ–‡æ•°æ®å·²ä¿å­˜: {output_file}")
    
    def process_images(self):
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶"""
        source_images_dir = self.source_dir / "extracted_content" / "images"
        if not source_images_dir.exists():
            print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•")
            return
        
        print("ğŸ–¼ï¸ å¤„ç†å›¾ç‰‡æ–‡ä»¶...")
        
        # å¤åˆ¶å‰100å¼ å›¾ç‰‡ä½œä¸ºç¤ºä¾‹
        image_files = list(source_images_dir.glob("*.png"))[:100]
        
        for i, img_file in enumerate(image_files):
            dest_file = self.images_dir / img_file.name
            try:
                shutil.copy2(img_file, dest_file)
                if i % 20 == 0:
                    print(f"  ğŸ“¸ å·²å¤åˆ¶ {i+1}/{len(image_files)} å¼ å›¾ç‰‡")
            except Exception as e:
                print(f"âŒ å¤åˆ¶å›¾ç‰‡å¤±è´¥ {img_file}: {e}")
        
        print(f"âœ… å·²å¤åˆ¶ {len(image_files)} å¼ å›¾ç‰‡")
    
    def generate_statistics(self, papers_data):
        """ç”Ÿæˆç»Ÿè®¡æ•°æ®"""
        papers = papers_data['papers']
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = {
            'total_papers': len(papers),
            'total_pages': sum(p['page_count'] for p in papers),
            'total_authors': len(set(author for p in papers for author in p['authors'])),
            'total_institutions': len(set(aff for p in papers for aff in p['affiliations'])),
            'total_figures': sum(p['figure_count'] for p in papers),
            'total_tables': sum(p['table_count'] for p in papers),
            'total_references': sum(p['reference_count'] for p in papers),
            'papers_with_abstract': sum(1 for p in papers if p['abstract']),
            'papers_with_keywords': sum(1 for p in papers if p['keywords']),
            'avg_pages': sum(p['page_count'] for p in papers) / len(papers) if papers else 0,
            'avg_figures': sum(p['figure_count'] for p in papers) / len(papers) if papers else 0,
            'avg_references': sum(p['reference_count'] for p in papers) / len(papers) if papers else 0
        }
        
        # æœºæ„ç»Ÿè®¡
        institution_counts = {}
        for paper in papers:
            for aff in paper['affiliations']:
                # ç®€åŒ–æœºæ„åç§°
                simple_name = aff.split(',')[0].strip()
                institution_counts[simple_name] = institution_counts.get(simple_name, 0) + 1
        
        top_institutions = sorted(institution_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # å…³é”®è¯ç»Ÿè®¡
        keyword_counts = {}
        for paper in papers:
            for keyword in paper['keywords']:
                keyword_counts[keyword.lower()] = keyword_counts.get(keyword.lower(), 0) + 1
        
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # é¡µæ•°åˆ†å¸ƒ
        page_distribution = {'1-2é¡µ': 0, '3-4é¡µ': 0, '5-6é¡µ': 0, '7-8é¡µ': 0, '8+é¡µ': 0}
        for paper in papers:
            pages = paper['page_count']
            if pages <= 2: page_distribution['1-2é¡µ'] += 1
            elif pages <= 4: page_distribution['3-4é¡µ'] += 1
            elif pages <= 6: page_distribution['5-6é¡µ'] += 1
            elif pages <= 8: page_distribution['7-8é¡µ'] += 1
            else: page_distribution['8+é¡µ'] += 1
        
        # å›¾ç‰‡åˆ†å¸ƒ
        figure_distribution = {'0-5å›¾': 0, '6-10å›¾': 0, '11-15å›¾': 0, '16-20å›¾': 0, '20+å›¾': 0}
        for paper in papers:
            figures = paper['figure_count']
            if figures <= 5: figure_distribution['0-5å›¾'] += 1
            elif figures <= 10: figure_distribution['6-10å›¾'] += 1
            elif figures <= 15: figure_distribution['11-15å›¾'] += 1
            elif figures <= 20: figure_distribution['16-20å›¾'] += 1
            else: figure_distribution['20+å›¾'] += 1
        
        complete_stats = {
            'basic_stats': stats,
            'top_institutions': top_institutions,
            'top_keywords': top_keywords,
            'page_distribution': page_distribution,
            'figure_distribution': figure_distribution,
            'generated_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = self.data_dir / "statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(complete_stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ç»Ÿè®¡æ•°æ®å·²ä¿å­˜: {stats_file}")
    
    def copy_static_files(self):
        """å¤åˆ¶å¿…è¦çš„é™æ€æ–‡ä»¶"""
        # åˆ›å»ºREADMEæ–‡ä»¶
        readme_content = f"""# HIAT2025 è®ºæ–‡å†…å®¹åˆ†æç³»ç»Ÿ

è¿™æ˜¯ä¸€ä¸ªåŸºäºGitHub Pagesçš„è®ºæ–‡å†…å®¹åˆ†æå’ŒæŸ¥è¯¢ç³»ç»Ÿã€‚

## æ•°æ®æ›´æ–°æ—¶é—´
{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## åŠŸèƒ½ç‰¹æ€§
- ğŸ“š 86ç¯‡è®ºæ–‡çš„è¯¦ç»†å†…å®¹åˆ†æ
- ğŸ” æ™ºèƒ½æœç´¢å’Œè¿‡æ»¤
- ğŸ“Š æ•°æ®å¯è§†åŒ–å›¾è¡¨
- ğŸ–¼ï¸ å›¾ç‰‡ç”»å»Šæµè§ˆ
- ğŸ“± å“åº”å¼è®¾è®¡

## æŠ€æœ¯æ ˆ
- å‰ç«¯: HTML5, CSS3, JavaScript, Bootstrap 5
- å›¾è¡¨: Chart.js
- éƒ¨ç½²: GitHub Pages
- æ•°æ®å¤„ç†: Python

## è®¿é—®åœ°å€
https://{os.environ.get('GITHUB_REPOSITORY_OWNER', 'username')}.github.io/{os.environ.get('GITHUB_REPOSITORY', 'repo').split('/')[-1]}/

## æ•°æ®æ¥æº
æ•°æ®æ¥æºäºHIAT2025é‡ç¦»å­åŠ é€Ÿå™¨æŠ€æœ¯å›½é™…ä¼šè®®è®ºæ–‡é›†çš„æ·±åº¦åˆ†æã€‚
"""
        
        readme_file = self.output_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"ğŸ“„ READMEå·²ç”Ÿæˆ: {readme_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HIAT2025 æ•°æ®å¤„ç†å™¨")
    print("=" * 50)
    
    processor = DataProcessor()
    
    if processor.process_all_data():
        print("\nğŸ‰ æ•°æ®å¤„ç†æˆåŠŸå®Œæˆ!")
        print("ğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        print("  docs/")
        print("  â”œâ”€â”€ index.html        # ä¸»é¡µé¢")
        print("  â”œâ”€â”€ app.js           # å‰ç«¯é€»è¾‘")
        print("  â”œâ”€â”€ data/")
        print("  â”‚   â”œâ”€â”€ papers.json  # è®ºæ–‡æ•°æ®")
        print("  â”‚   â””â”€â”€ statistics.json # ç»Ÿè®¡æ•°æ®")
        print("  â””â”€â”€ images/          # å›¾ç‰‡æ–‡ä»¶")
        print("\nğŸŒ å‡†å¤‡éƒ¨ç½²åˆ°GitHub Pages!")
    else:
        print("\nâŒ æ•°æ®å¤„ç†å¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main()
