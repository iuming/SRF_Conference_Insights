#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨æ­£ç¡®PDF URLçš„è¯¦ç»†ä¿¡æ¯çˆ¬è™«
"""

import requests
import PyPDF2
import io
import json
import re
from datetime import datetime
from typing import List, Dict, Set

def extract_detailed_info_from_pdfs():
    """ä»æ­£ç¡®çš„PDFæ–‡ä»¶ä¸­æå–è¯¦ç»†ä¿¡æ¯"""
    print("ğŸ¯ ä½¿ç”¨æ­£ç¡®PDF URLæå–è¯¦ç»†ä¿¡æ¯")
    print("=" * 50)
    
    # æ­£ç¡®çš„PDF URL
    pdf_urls = [
        ("å®Œæ•´è®ºæ–‡é›†", "https://meow.elettra.eu/82/pdf/hiat2025_proceedings_volume.pdf"),
        ("è®ºæ–‡æ¦‚è§ˆ", "https://meow.elettra.eu/82/pdf/hiat2025_proceedings_brief.pdf")
    ]
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    results = {
        "conference_info": {
            "name": "HIAT2025",
            "full_name": "16th International Conference on Heavy Ion Accelerator Technology",
            "date": "22-27 June 2025",
            "location": "Michigan State University, East Lansing, USA",
            "host": "Facility for Rare Isotope Beams (FRIB)"
        },
        "extracted_authors": [],
        "extracted_institutions": [],
        "extracted_papers": [],
        "technical_keywords": [],
        "pdf_analysis": {}
    }
    
    for pdf_name, pdf_url in pdf_urls:
        print(f"\nğŸ“š åˆ†æ {pdf_name}")
        print(f"URL: {pdf_url}")
        
        try:
            response = session.get(pdf_url, timeout=60)
            print(f"HTTPçŠ¶æ€: {response.status_code}")
            print(f"æ–‡ä»¶å¤§å°: {len(response.content):,} å­—èŠ‚")
            
            if response.status_code == 200 and len(response.content) > 100000:  # è‡³å°‘100KB
                pdf_file = io.BytesIO(response.content)
                reader = PyPDF2.PdfReader(pdf_file)
                
                total_pages = len(reader.pages)
                print(f"PDFé¡µæ•°: {total_pages}")
                
                # åˆ†æç­–ç•¥ï¼šåˆ†æå‰20é¡µè·å–ä½œè€…å’Œè®ºæ–‡ä¿¡æ¯
                pages_to_analyze = min(20, total_pages)
                print(f"åˆ†æé¡µæ•°: {pages_to_analyze}")
                
                all_text = ""
                authors_found = set()
                institutions_found = set()
                papers_found = []
                
                for page_num in range(pages_to_analyze):
                    try:
                        page = reader.pages[page_num]
                        page_text = page.extract_text()
                        
                        if page_text and len(page_text.strip()) > 50:
                            all_text += page_text + "\n"
                            
                            # ä»æ¯é¡µæå–ä¿¡æ¯
                            page_authors = extract_authors_from_page(page_text)
                            page_institutions = extract_institutions_from_page(page_text)
                            page_papers = extract_papers_from_page(page_text)
                            
                            authors_found.update(page_authors)
                            institutions_found.update(page_institutions)
                            papers_found.extend(page_papers)
                            
                            if page_num < 5:  # æ˜¾ç¤ºå‰5é¡µçš„æ ·æœ¬
                                print(f"   é¡µé¢{page_num+1}: {len(page_text)} å­—ç¬¦")
                                
                    except Exception as e:
                        print(f"   é¡µé¢{page_num+1}å¤„ç†å¤±è´¥: {str(e)}")
                        continue
                
                # æ±‡æ€»åˆ†æç»“æœ
                pdf_analysis = {
                    "name": pdf_name,
                    "url": pdf_url,
                    "total_pages": total_pages,
                    "analyzed_pages": pages_to_analyze,
                    "total_text_length": len(all_text),
                    "authors_count": len(authors_found),
                    "institutions_count": len(institutions_found),
                    "papers_count": len(papers_found)
                }
                
                results["pdf_analysis"][pdf_name] = pdf_analysis
                results["extracted_authors"].extend(list(authors_found))
                results["extracted_institutions"].extend(list(institutions_found))
                results["extracted_papers"].extend(papers_found)
                
                print(f"âœ… {pdf_name}åˆ†æå®Œæˆ:")
                print(f"   ä½œè€…: {len(authors_found)}")
                print(f"   æœºæ„: {len(institutions_found)}")
                print(f"   è®ºæ–‡: {len(papers_found)}")
                
                # ä¿å­˜æ–‡æœ¬æ ·æœ¬
                if all_text.strip():
                    sample_file = f"pdf_content_{pdf_name.replace(' ', '_')}.txt"
                    with open(sample_file, 'w', encoding='utf-8') as f:
                        f.write(f"PDF: {pdf_name}\nURL: {pdf_url}\n\n")
                        f.write(all_text[:5000])  # ä¿å­˜å‰5000å­—ç¬¦
                    print(f"   æ–‡æœ¬æ ·æœ¬å·²ä¿å­˜: {sample_file}")
                
            else:
                print(f"âŒ PDFè®¿é—®å¤±è´¥: çŠ¶æ€{response.status_code}, å¤§å°{len(response.content)}")
                
        except Exception as e:
            print(f"âŒ PDFå¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    # å»é‡å’Œæ¸…ç†
    unique_authors = list(set(results["extracted_authors"]))
    unique_institutions = list(set(results["extracted_institutions"]))
    
    # è¿‡æ»¤å’ŒéªŒè¯
    verified_authors = [author for author in unique_authors if is_valid_author_name(author)]
    verified_institutions = [inst for inst in unique_institutions if is_valid_institution_name(inst)]
    
    results["verified_authors"] = verified_authors
    results["verified_institutions"] = verified_institutions
    
    # æå–æŠ€æœ¯å…³é”®è¯
    physics_keywords = [
        'accelerator', 'beam', 'ion', 'heavy ion', 'proton', 'electron',
        'synchrotron', 'cyclotron', 'linac', 'radiofrequency', 'rf',
        'superconducting', 'magnet', 'cavity', 'injection', 'extraction',
        'collider', 'isotope', 'radioisotope', 'neutron', 'target'
    ]
    
    found_keywords = []
    all_content = ' '.join([results["pdf_analysis"].get(pdf, {}).get("name", "") for pdf in results["pdf_analysis"]])
    
    for keyword in physics_keywords:
        if keyword in all_content.lower():
            found_keywords.append(keyword)
    
    results["technical_keywords"] = found_keywords
    
    # ç”Ÿæˆç»Ÿè®¡
    results["statistics"] = {
        "total_verified_authors": len(verified_authors),
        "total_verified_institutions": len(verified_institutions),
        "total_technical_keywords": len(found_keywords),
        "total_papers": len(results["extracted_papers"]),
        "pdf_files_processed": len(results["pdf_analysis"]),
        "extraction_timestamp": datetime.now().isoformat()
    }
    
    return results

def extract_authors_from_page(text: str) -> Set[str]:
    """ä»é¡µé¢æ–‡æœ¬ä¸­æå–ä½œè€…"""
    authors = set()
    
    # ä½œè€…å§“åæ¨¡å¼
    patterns = [
        r'\b([A-Z][a-z]{2,15}\s+[A-Z][a-z]{2,15})\b',  # John Smith
        r'\b([A-Z][a-z]{2,15}\s+[A-Z]\.\s+[A-Z][a-z]{2,15})\b',  # John A. Smith
        r'\b([A-Z]\.\s*[A-Z][a-z]{2,15})\b',  # A. Smith
        r'\b([A-Z][a-z]{2,15},\s*[A-Z][a-z]{2,15})\b'  # Smith, John
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if is_valid_author_name(match):
                authors.add(match.strip())
    
    return authors

def extract_institutions_from_page(text: str) -> Set[str]:
    """ä»é¡µé¢æ–‡æœ¬ä¸­æå–æœºæ„"""
    institutions = set()
    
    patterns = [
        r'\b([A-Z][a-zA-Z\s]{5,40}\s+University)\b',
        r'\b(University\s+of\s+[A-Z][a-zA-Z\s]{3,30})\b',
        r'\b([A-Z][a-zA-Z\s]{5,40}\s+Institute)\b',
        r'\b([A-Z][a-zA-Z\s]{5,40}\s+Laboratory)\b',
        r'\b(Michigan State University)\b',
        r'\b(FRIB|NSCL|CERN|DESY|KEK|SLAC|Fermilab)\b'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if is_valid_institution_name(match):
                institutions.add(match.strip())
    
    return institutions

def extract_papers_from_page(text: str) -> List[Dict]:
    """ä»é¡µé¢æ–‡æœ¬ä¸­æå–è®ºæ–‡æ ‡é¢˜"""
    papers = []
    
    # æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯è¾ƒé•¿çš„å¤§å†™æˆ–æ ‡é¢˜æ ¼å¼æ–‡æœ¬ï¼‰
    title_patterns = [
        r'\n([A-Z][A-Z\s\-:]{20,100})\n',
        r'\n([A-Z][a-zA-Z\s\-:]{25,120})\n'
    ]
    
    for pattern in title_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            title = match.strip()
            if is_valid_paper_title(title):
                papers.append({
                    "title": title,
                    "source": "pdf_extraction"
                })
    
    return papers

def is_valid_author_name(name: str) -> bool:
    """éªŒè¯ä½œè€…å§“å"""
    if not name or len(name) < 4 or len(name) > 50:
        return False
    
    # æ’é™¤æ˜æ˜¾ä¸æ˜¯å§“åçš„è¯
    invalid_words = [
        'Abstract', 'Conference', 'Proceedings', 'Volume', 'Session',
        'Committee', 'Program', 'Schedule', 'Heavy Ion', 'Accelerator',
        'Technology', 'Physics', 'Publishing', 'Creative Commons'
    ]
    
    for invalid in invalid_words:
        if invalid.lower() in name.lower():
            return False
    
    # æ£€æŸ¥å§“åç»“æ„
    parts = name.split()
    if len(parts) < 2:
        return False
    
    # æ¯ä¸ªéƒ¨åˆ†åº”è¯¥ä»¥å¤§å†™å­—æ¯å¼€å¤´
    for part in parts:
        if not part[0].isupper():
            return False
    
    return True

def is_valid_institution_name(name: str) -> bool:
    """éªŒè¯æœºæ„åç§°"""
    if not name or len(name) < 5 or len(name) > 100:
        return False
    
    institution_keywords = [
        'University', 'Institute', 'Laboratory', 'Lab', 'Center',
        'College', 'School', 'FRIB', 'NSCL', 'CERN', 'DESY'
    ]
    
    return any(keyword in name for keyword in institution_keywords)

def is_valid_paper_title(title: str) -> bool:
    """éªŒè¯è®ºæ–‡æ ‡é¢˜"""
    if not title or len(title) < 15 or len(title) > 200:
        return False
    
    invalid_indicators = [
        'Page', 'Table', 'Figure', 'Abstract', 'Session',
        'Track', 'Program', 'Schedule', 'Proceedings'
    ]
    
    return not any(indicator in title for indicator in invalid_indicators)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HIAT2025 PDFè¯¦ç»†ä¿¡æ¯æå–å™¨")
    print("ä½¿ç”¨æ­£ç¡®çš„PDF URLè·å–ä½œè€…ã€æœºæ„å’Œè®ºæ–‡ä¿¡æ¯")
    print()
    
    try:
        results = extract_detailed_info_from_pdfs()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"hiat2025_pdf_extraction_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºæ€»ç»“
        stats = results["statistics"]
        print(f"\nğŸ‰ PDFä¿¡æ¯æå–å®Œæˆï¼")
        print("=" * 40)
        print(f"ğŸ“Š éªŒè¯ä½œè€…: {stats['total_verified_authors']} ä½")
        print(f"ğŸ›ï¸ éªŒè¯æœºæ„: {stats['total_verified_institutions']} ä¸ª")
        print(f"ğŸ”¬ æŠ€æœ¯å…³é”®è¯: {stats['total_technical_keywords']} ä¸ª")
        print(f"ğŸ“„ è®ºæ–‡æ ‡é¢˜: {stats['total_papers']} ç¯‡")
        print(f"ğŸ“š å¤„ç†PDF: {stats['pdf_files_processed']} ä¸ª")
        print()
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {result_file}")
        
        if stats['total_verified_authors'] > 0:
            print("\nğŸ‘¥ éªŒè¯ä½œè€…ç¤ºä¾‹ (å‰10ä½):")
            for i, author in enumerate(results["verified_authors"][:10], 1):
                print(f"   {i}. {author}")
        
        if stats['total_verified_institutions'] > 0:
            print("\nğŸ›ï¸ éªŒè¯æœºæ„ç¤ºä¾‹ (å‰5ä¸ª):")
            for i, inst in enumerate(results["verified_institutions"][:5], 1):
                print(f"   {i}. {inst}")
        
        print(f"\nğŸŒŸ æˆåŠŸä»PDFæ–‡æ¡£ä¸­æå–äº†è¯¦ç»†çš„ä¼šè®®ä¿¡æ¯ï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒPyPDF2å®‰è£…")

if __name__ == "__main__":
    main()
